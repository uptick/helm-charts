apiVersion: batch/v1
kind: CronJob
metadata:
  name: {{ include "pod-killer.fullname" . }}
spec:
  # once an hour
  schedule: {{ .Values.cron.schedule | quote}}
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        spec:
          serviceAccount: {{ include "pod-killer.serviceAccountName" . }}
          restartPolicy: Never
          containers:
          - name: pod-killer
            image: bitnami/kubectl
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            args:
            - -c
            - |
              echo Deleting #{{.Values.podKiller.nPods}} random pods older than {{ .Values.podKiller.maxPodAgeHours }} hours
              PODS_TO_KILL=$(kubectl get pods --selector 'app.kubernetes.io/name=workforce-fg' -o json \
              | jq -r \
                '.items[] | select((.metadata.creationTimestamp | fromdate) < (now - ( {{.Values.podKiller.maxPodAgeHours}} * 60 * 60 )))| select(.spec.priority >= 2000) ' \
              | jq -s -r '. | map( { (.metadata.labels.app): .metadata.name } ) | add | to_entries[] | .value' \
              | shuf -n  {{ .Values.podKiller.nPods }})

              kubectl delete pods $PODS_TO_KILL || true

              # Deleting evicted pods
              PODS_TO_EVICT=$(kubectl get pod --field-selector=status.phase=Failed -o json | jq -r '.items[] | select(.status.reason=="Evicted") | .metadata.name')
              echo "Deleting Evicted pods"

              kubectl delete pods $(echo $PODS_TO_EVICT) || true

              # Deleting OLD jobs
              JOBS_TO_KILL=$(kubectl get jobs -o json | jq '.items[] | select((.metadata.creationTimestamp | fromdate) < (now - (10*60*60)))' | jq '.metadata.name' -r)
              kubectl delete jobs $(echo $JOBS_TO_KILL) || true


# We are getting all foreground workforce pods
# That are over 4 hours alive
# That are priority over 2000 (customer pods with more than 1 pod). We don't want to kill sandbox pods since they will go down
# only 1 per set of deployment
# and randomise

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "pod-killer.serviceAccountName" . }}
  labels:
    {{- include "pod-killer.labels" . | nindent 4 }}

---

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: {{ include "pod-killer.fullname" . }}
rules:
- apiGroups: ["", "batch"] # "" indicates the core API group
  resources: ["pods", "jobs"]
  verbs: ["get", "list", "delete", "watch"]


---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: {{ include "pod-killer.fullname" . }}
subjects:
- kind: ServiceAccount
  name:  {{ include "pod-killer.serviceAccountName" . }}
  namespace: {{ .Release.Namespace }}
roleRef:
  # "roleRef" specifies the binding to a Role / ClusterRole
  kind: Role #this must be Role or ClusterRole
  name: {{ include "pod-killer.fullname" . }}
  apiGroup: rbac.authorization.k8s.io
